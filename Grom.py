import streamlit as st
import pandas as pd
import numpy as np
import json
import requests
import datetime
import folium
from streamlit_folium import st_folium

st.set_page_config(page_title="Ğ—Ğ»Ğ¾Ñ‡Ğ¸Ğ½Ğ½Ñ–ÑÑ‚ÑŒ Ñƒ Ñ€ĞµĞ³Ñ–Ğ¾Ğ½Ğ°Ñ…", layout="wide")

# -----------------------------
# ğŸ“¦ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ´Ğ°Ğ½Ğ¸Ñ…
# -----------------------------
@st.cache_data
def load_csv(filelike):
    return pd.read_csv(filelike)

@st.cache_data
def load_csv_from_url(url: str):
    return pd.read_csv(url)

@st.cache_data
def load_geojson(url_or_path: str):
    try:
        if str(url_or_path).startswith("http"):
            r = requests.get(url_or_path, timeout=10)
            r.raise_for_status()
            return r.json()
        else:
            with open(url_or_path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        st.error(f"ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ GeoJSON: {e}")
        return None

# -----------------------------
# âš™ï¸ ĞŸĞ¾Ğ¿ĞµÑ€ĞµĞ´Ğ½Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ°
# -----------------------------
def preprocess(df, date_col='date', category_col='category', region_col='region'):
    df = df.copy()
    df.columns = [c.strip() for c in df.columns]
    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    df = df.dropna(subset=[date_col])
    df[category_col] = df[category_col].astype(str)
    df[region_col] = df[region_col].astype(str)
    df['month'] = df[date_col].dt.to_period('M').dt.to_timestamp()
    return df

def compute_crime_index(df, region_col='region', date_col='month'):
    agg = df.groupby([region_col, date_col]).size().reset_index(name='count')
    total = agg.groupby(region_col)['count'].sum().reset_index()
    minv, maxv = total['count'].min(), total['count'].max()
    total['crime_index'] = (
        (total['count'] - minv) / (maxv - minv) * 100 if maxv != minv else 50
    )
    return total

# -----------------------------
# ğŸ§­ Ğ†Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ°
# -----------------------------
st.sidebar.header("ğŸ“Š Ğ”Ğ¶ĞµÑ€ĞµĞ»Ğ¾ Ğ´Ğ°Ğ½Ğ¸Ñ…")
source = st.sidebar.radio("ĞĞ±ĞµÑ€Ñ–Ñ‚ÑŒ:", ["ĞŸÑ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ– Ğ´Ğ°Ğ½Ñ–", "Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ CSV", "Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ğ· URL"])

if source == "ĞŸÑ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ– Ğ´Ğ°Ğ½Ñ–":
    # Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ¸Ñ…
    rng = pd.date_range(end=pd.Timestamp.today(), periods=365)
    cats = ['ĞšÑ€Ğ°Ğ´Ñ–Ğ¶ĞºĞ°', 'Ğ¨Ğ°Ñ…Ñ€Ğ°Ğ¹ÑÑ‚Ğ²Ğ¾', 'ĞĞ°ÑĞ¸Ğ»ÑŒÑÑ‚Ğ²Ğ¾', 'ĞĞ°Ñ€ĞºĞ¾Ñ‚Ğ¸ĞºĞ¸']
    regions = ['ĞšĞ¸Ñ—Ğ²ÑÑŒĞºĞ°', 'Ğ›ÑŒĞ²Ñ–Ğ²ÑÑŒĞºĞ°', 'ĞĞ´ĞµÑÑŒĞºĞ°', 'Ğ¥Ğ°Ñ€ĞºÑ–Ğ²ÑÑŒĞºĞ°', 'Ğ”Ğ½Ñ–Ğ¿Ñ€Ğ¾Ğ¿ĞµÑ‚Ñ€Ğ¾Ğ²ÑÑŒĞºĞ°']
    np.random.seed(42)
    data = []
    for d in rng:
        for r in regions:
            n = np.random.poisson(lam=2)
            for _ in range(n):
                data.append({
                    'category': np.random.choice(cats),
                    'region': r,
                    'date': d + pd.to_timedelta(np.random.randint(0, 24), unit='h')
                })
    df = pd.DataFrame(data)

elif source == "Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ CSV":
    uploaded = st.sidebar.file_uploader("Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ñ‚Ğµ CSV (category, region, date)", type=["csv"])
    if not uploaded:
        st.stop()
    df = load_csv(uploaded)

else:
    url = st.sidebar.text_input("Ğ’Ğ²ĞµĞ´Ñ–Ñ‚ÑŒ URL Ğ´Ğ¾ CSV")
    if not url:
        st.stop()
    df = load_csv_from_url(url)

# -----------------------------
# ğŸ§¹ ĞŸÑ–Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ°
# -----------------------------
st.sidebar.markdown("---")
st.sidebar.header("ĞĞ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ ÑÑ‚Ğ¾Ğ²Ğ¿Ñ†Ñ–Ğ²")
date_col = st.sidebar.text_input("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ· Ğ´Ğ°Ñ‚Ğ¾Ñ", "date")
category_col = st.sidebar.text_input("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ”Ñ", "category")
region_col = st.sidebar.text_input("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ· Ñ€ĞµĞ³Ñ–Ğ¾Ğ½Ğ¾Ğ¼", "region")

df = preprocess(df, date_col, category_col, region_col)

# -----------------------------
# ğŸ§© Ğ¤Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸
# -----------------------------
st.title("ğŸš“ Ğ—Ğ»Ğ¾Ñ‡Ğ¸Ğ½Ğ½Ñ–ÑÑ‚ÑŒ Ñƒ Ñ€ĞµĞ³Ñ–Ğ¾Ğ½Ğ°Ñ…")
min_date, max_date = df[date_col].min().date(), df[date_col].max().date()
date_range = st.date_input("Ğ”Ñ–Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚", (min_date, max_date))

selected_categories = st.multiselect("ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ—", df[category_col].unique(), df[category_col].unique())
selected_regions = st.multiselect("Ğ ĞµĞ³Ñ–Ğ¾Ğ½Ğ¸", df[region_col].unique(), df[region_col].unique())

mask = (
    (df[date_col] >= pd.to_datetime(date_range[0])) &
    (df[date_col] <= pd.to_datetime(date_range[1])) &
    (df[category_col].isin(selected_categories)) &
    (df[region_col].isin(selected_regions))
)
df_filtered = df[mask]

st.write(f"Ğ’Ñ–Ğ´Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ {len(df_filtered)} Ğ·Ğ°Ğ¿Ğ¸ÑÑ–Ğ²")

# -----------------------------
# ğŸ“ˆ Ğ’Ñ–Ğ·ÑƒĞ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ñ–ĞºĞ¸
# -----------------------------
st.subheader("ğŸ“… Ğ”Ğ¸Ğ½Ğ°Ğ¼Ñ–ĞºĞ° Ğ·Ğ»Ğ¾Ñ‡Ğ¸Ğ½Ñ–Ğ²")
trend = df_filtered.groupby(df_filtered[date_col].dt.to_period('M')).size()
st.line_chart(trend)

st.subheader("ğŸ“‚ Ğ Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ» Ğ·Ğ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–ÑĞ¼Ğ¸")
cat_count = df_filtered[category_col].value_counts()
st.bar_chart(cat_count)

# -----------------------------
# ğŸ”¢ Ğ†Ğ½Ğ´ĞµĞºÑ ĞºÑ€Ğ¸Ğ¼Ñ–Ğ½Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ñ–
# -----------------------------
st.subheader("ğŸ’¥ Ğ†Ğ½Ğ´ĞµĞºÑ ĞºÑ€Ğ¸Ğ¼Ñ–Ğ½Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ñ– Ğ¿Ğ¾ Ñ€ĞµĞ³Ñ–Ğ¾Ğ½Ğ°Ñ…")
index_df = compute_crime_index(df_filtered, region_col, date_col)
st.dataframe(index_df.sort_values('crime_index', ascending=False), use_container_width=True)

# -----------------------------
# ğŸ—ºï¸ ĞšĞ°Ñ€Ñ‚Ğ°
# -----------------------------
st.subheader("ğŸ—ºï¸ ĞšĞ°Ñ€Ñ‚Ğ° Ğ·Ğ»Ğ¾Ñ‡Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ñ–")
geojson_url = st.text_input("URL Ğ´Ğ¾ GeoJSON ĞºĞ°Ñ€Ñ‚Ğ¸ (Ğ¾Ğ¿Ñ†Ñ–Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)")
if geojson_url:
    geojson_data = load_geojson(geojson_url)
    if geojson_data:
        m = folium.Map(location=[48.3794, 31.1656], zoom_start=6)
        folium.Choropleth(
            geo_data=geojson_data,
            name="choropleth",
            data=index_df,
            columns=[region_col, 'crime_index'],
            key_on="feature.properties.name",
            fill_color="YlOrRd",
            fill_opacity=0.7,
            line_opacity=0.2,
            legend_name="Ğ†Ğ½Ğ´ĞµĞºÑ ĞºÑ€Ğ¸Ğ¼Ñ–Ğ½Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ñ– (0â€“100)"
        ).add_to(m)
        st_folium(m, width=700, height=500)
else:
    region_summary = index_df.set_index(region_col)['crime_index']
    st.bar_chart(region_summary)

# -----------------------------
# ğŸ’¾ Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚
# -----------------------------
st.subheader("ğŸ’¾ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸")
csv_data = df_filtered.to_csv(index=False).encode('utf-8')
st.download_button("â¬‡ï¸ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ CSV", data=csv_data, file_name="filtered_crime_data.csv", mime="text/csv")

st.markdown("---")
st.caption("Ğ—Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ· Ğ´Ğ¾Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ¾Ñ GPT Online â€” [https://gptonline.ai/](https://gptonline.ai/)")
